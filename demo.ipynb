{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "import scipy\n",
    "import brian2 as b\n",
    "from brian2 import *\n",
    "import time\n",
    "from math import *\n",
    "\n",
    "from Functions import *\n",
    "\n",
    "# set parameters\n",
    "np.random.seed(0)\n",
    "prefs.codegen.target = 'auto'\n",
    "prefs.codegen.cpp.extra_compile_args_gcc = ['-march=native']\n",
    "\n",
    "# ---------------------------------------------------Build network----------------------------------------------------------\n",
    "Learning = False\n",
    "\n",
    "print('The settings of network are as follow:')\n",
    "print('---------------------------------------------')\n",
    "\n",
    "# the amount of input\n",
    "n_input = 784\n",
    "\n",
    "# connection parameters\n",
    "kernel_type_num = 3\n",
    "kernel_size_each_type = [28, 24, 16]\n",
    "stride_each_type = [1, 4, 6]\n",
    "feature_map_size_each_type = [int(\n",
    "    ((sqrt(n_input) - kernel_size_each_type[i]) / stride_each_type[i] + 1)**2) for i in range(kernel_type_num)]\n",
    "print('Num of kernel type:', kernel_type_num)\n",
    "print('Kernel size and Stride of each kernel type:',\n",
    "      kernel_size_each_type, stride_each_type)\n",
    "print('Feature map size of each kernel type:', feature_map_size_each_type)\n",
    "\n",
    "feature_map_num = 448\n",
    "kernel_num_each_type = [4, 2, 1]\n",
    "kernel_num = np.sum(kernel_num_each_type)\n",
    "print('Feature map num:', feature_map_num)\n",
    "print('Kernel num of each kernel type:', kernel_num_each_type)\n",
    "print('Num of kernel:', kernel_num)\n",
    "\n",
    "# the amount of neurons\n",
    "neuron_num_each_kernel = []\n",
    "feature_map_size_each_kernel = []\n",
    "kernel_size_each_kernel = []\n",
    "stride_each_kernel = []\n",
    "for kernel_type in range(kernel_type_num):\n",
    "    for kernel in range(kernel_num_each_type[kernel_type]):\n",
    "        neuron_num_each_kernel.append(\n",
    "            feature_map_num * feature_map_size_each_type[kernel_type])\n",
    "        feature_map_size_each_kernel.append(\n",
    "            feature_map_size_each_type[kernel_type])\n",
    "        kernel_size_each_kernel.append(kernel_size_each_type[kernel_type])\n",
    "        stride_each_kernel.append(stride_each_type[kernel_type])\n",
    "\n",
    "neuron_num = np.sum(neuron_num_each_kernel)\n",
    "print('Neurons num of each kernel:', neuron_num_each_kernel)\n",
    "print('Num of Neurons:', neuron_num)\n",
    "print('---------------------------------------------')\n",
    "\n",
    "# neuron parameters\n",
    "v_rest_e = -65. * b.mV\n",
    "v_reset_e = -65. * b.mV\n",
    "v_thresh_e = -52. * b.mV\n",
    "refrac_e = 5. * b.ms\n",
    "\n",
    "# synapses parameters\n",
    "Delay = 10*b.ms\n",
    "tc_pre_ee = 20*b.ms\n",
    "tc_post_1_ee = 20*b.ms\n",
    "tc_post_2_ee = 40*b.ms\n",
    "nu_ee_pre = 0.0001\n",
    "nu_ee_post = 0.01\n",
    "wmax_ee = 1.0\n",
    "wmin_ee = 1e-7\n",
    "ihn = 24\n",
    "norm = 78.4\n",
    "\n",
    "if Learning == False:\n",
    "    scr_e = 'v = v_reset_e'\n",
    "else:\n",
    "    tc_theta = 1e7 * b.ms\n",
    "    theta_plus_e = 0.05 * b.mV\n",
    "    scr_e = 'v = v_reset_e; theta += theta_plus_e'\n",
    "offset = 20.0*b.mV\n",
    "thresh_e = 'v>(theta - offset + ' + str(v_thresh_e/b.mV) + '*mV' + ')'\n",
    "\n",
    "# equation of excitatory neurons\n",
    "neuron_eqs_e = '''\n",
    "            dv/dt = ((v_rest_e - v) + (I_synE+I_synI) / nS) / (100*ms)  : volt\n",
    "            I_synE = ge * nS * (        -v)                             : amp\n",
    "            I_synI = gi * nS * (-100.*mV-v)                             : amp\n",
    "            dge/dt = -ge/(1.0*ms)                                       : 1\n",
    "            dgi/dt = -gi/(2.0*ms)                                       : 1\n",
    "            '''\n",
    "if Learning == False:\n",
    "    neuron_eqs_e += '\\n  theta      :volt'\n",
    "else:\n",
    "    neuron_eqs_e += '\\n  dtheta/dt = -theta / (tc_theta)  : volt'\n",
    "\n",
    "# equation of STDP\n",
    "eqs_stdp_ee = '''\n",
    "                    w                                      : 1\n",
    "                    post2before                            : 1\n",
    "                    dpre/dt    = -pre/(tc_pre_ee)          : 1 (event-driven)\n",
    "                    dpost1/dt  = -post1/(tc_post_1_ee)     : 1 (event-driven)\n",
    "                    dpost2/dt  = -post2/(tc_post_2_ee)     : 1 (event-driven)\n",
    "                '''\n",
    "eqs_stdp_pre_ee = 'ge+=w; pre = 1.; w = (w>0)*clip(w - nu_ee_pre * post1 , wmin_ee, wmax_ee)'\n",
    "eqs_stdp_post_ee = 'post2before = post2; w = (w>0)*clip(w + nu_ee_post * pre * post2before, wmin_ee, wmax_ee); post1 = 1.; post2 = 1.'\n",
    "\n",
    "# create empty dict\n",
    "neuron_groups = {}\n",
    "connections = {}\n",
    "spike_counters = {}\n",
    "net = {}\n",
    "\n",
    "# create neuron group\n",
    "neuron_groups['X1'] = b.PoissonGroup(n_input, 0*b.hertz)\n",
    "neuron_groups['A1'] = b.NeuronGroup(\n",
    "    neuron_num, neuron_eqs_e, method='euler', threshold=thresh_e, refractory=refrac_e, reset=scr_e)\n",
    "neuron_groups['A1'].v = v_rest_e - 40. * b.mV\n",
    "neuron_groups['A1'].theta = np.ones((neuron_num)) * 20.0*b.mV\n",
    "\n",
    "# create connections AA\n",
    "start = time.time()\n",
    "weightMatrix = np.zeros((neuron_num, neuron_num))\n",
    "mark = 0\n",
    "for kernel in range(kernel_num):\n",
    "    feature_map_size = feature_map_size_each_kernel[kernel]\n",
    "    for src in range(mark, mark+neuron_num_each_kernel[kernel]):\n",
    "        S = src - mark\n",
    "        src_z = int(S/feature_map_size)\n",
    "        src_y = int((S - src_z*feature_map_size) / sqrt(feature_map_size))\n",
    "        src_x = int(S - src_z*feature_map_size - src_y*sqrt(feature_map_size))\n",
    "        for tar in range(mark, mark+neuron_num_each_kernel[kernel]):\n",
    "            T = tar - mark\n",
    "            tar_z = int(T / feature_map_size)\n",
    "            tar_y = int((T - tar_z*feature_map_size) / sqrt(feature_map_size))\n",
    "            tar_x = int(T - tar_z*feature_map_size -\n",
    "                        tar_y*sqrt(feature_map_size))\n",
    "            if src_x == tar_x and src_y == tar_y and src_z != tar_z:\n",
    "                weightMatrix[src, tar] = ihn\n",
    "    mark += neuron_num_each_kernel[kernel]\n",
    "weightMatrix = weightMatrix.reshape((neuron_num*neuron_num))\n",
    "connections['A1A1'] = b.Synapses(\n",
    "    neuron_groups['A1'], neuron_groups['A1'], 'w:1', on_pre='gi+=w')\n",
    "connections['A1A1'].connect()\n",
    "connections['A1A1'].w = weightMatrix\n",
    "end = time.time()\n",
    "print('time needed to create connection A1A1:', end - start)\n",
    "\n",
    "# create connections XA\n",
    "start = time.time()\n",
    "weightMatrix = np.zeros((n_input, neuron_num))\n",
    "if Learning:\n",
    "    mark = 0\n",
    "    for kernel in range(kernel_num):\n",
    "        feature_map_size = feature_map_size_each_kernel[kernel]\n",
    "        kernel_size = kernel_size_each_kernel[kernel]\n",
    "        stride = stride_each_kernel[kernel]\n",
    "        for src in range(n_input):\n",
    "            src_z = int(src / n_input)\n",
    "            src_y = int((src - src_z*n_input) / sqrt(n_input))\n",
    "            src_x = int(src - src_z*n_input - src_y*sqrt(n_input))\n",
    "            for tar in range(mark, mark+neuron_num_each_kernel[kernel]):\n",
    "                T = tar - mark\n",
    "                tar_z = int(T / feature_map_size)\n",
    "                tar_y = int((T - tar_z*feature_map_size) /\n",
    "                            sqrt(feature_map_size))\n",
    "                tar_x = int(T - tar_z*feature_map_size -\n",
    "                            tar_y*sqrt(feature_map_size))\n",
    "                if src_x >= tar_x*stride and src_x < tar_x*stride+kernel_size and src_y >= tar_y*stride and src_y < tar_y*stride+kernel_size:\n",
    "                    weightMatrix[src, tar] = 0.3*rand()+wmin_ee\n",
    "        mark += neuron_num_each_kernel[kernel]\n",
    "weightMatrix = weightMatrix.reshape((n_input*neuron_num))\n",
    "\n",
    "if Learning:\n",
    "    connections['X1A1'] = b.Synapses(neuron_groups['X1'], neuron_groups['A1'],\n",
    "                                     eqs_stdp_ee, on_pre=eqs_stdp_pre_ee, on_post=eqs_stdp_post_ee)\n",
    "else:\n",
    "    connections['X1A1'] = b.Synapses(\n",
    "        neuron_groups['X1'], neuron_groups['A1'], 'w : 1', on_pre='ge+=w')\n",
    "connections['X1A1'].connect()\n",
    "connections['X1A1'].w = weightMatrix\n",
    "connections['X1A1'].delay = 'rand()*' + str(Delay/b.ms) + '*ms'\n",
    "end = time.time()\n",
    "print('time needed to create connection X1A1:', end - start)\n",
    "\n",
    "# create monitors\n",
    "spike_counters['A1'] = b.SpikeMonitor(neuron_groups['A1'], record=False)\n",
    "\n",
    "# create networks\n",
    "net['M1'] = Network(neuron_groups['A1'], neuron_groups['X1'],\n",
    "                    connections['X1A1'], connections['A1A1'], spike_counters['A1'])\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# load MNIST\n",
    "start = time.time()\n",
    "training = get_labeled_data('./training')\n",
    "end = time.time()\n",
    "print('time needed to load training set:', end - start)\n",
    "\n",
    "# specify the location\n",
    "save_path = './weights/'\n",
    "load_path = './weights/'\n",
    "\n",
    "# the time-window of simulation\n",
    "single_example_time = 0.35 * b.second\n",
    "resting_time = 0.15 * b.second\n",
    "\n",
    "# the the interval of process data and show information\n",
    "progress_interval = 10\n",
    "validate_interval = 5000  # no less than 2000\n",
    "save_interval = 500\n",
    "\n",
    "# number of samples for training\n",
    "n_train = 60000\n",
    "train_begin = 0  # specify which iteration you want the training to begin from\n",
    "\n",
    "# load trained weight to continue\n",
    "if train_begin:\n",
    "    connections['X1A1'].w = np.load(\n",
    "        load_path + 'X1A1' + '_' + str(train_begin) + '.npy')\n",
    "    neuron_groups['A1'].theta = np.load(\n",
    "        load_path + 'theta_A1' + '_' + str(train_begin) + '.npy') * b.volt\n",
    "\n",
    "# the intensity of rate coding\n",
    "intensity_step = 0.125\n",
    "start_intensity = 0.25\n",
    "\n",
    "# the threshold of retrain\n",
    "retrain_gate = np.sum([5*feature_map_size_each_kernel[kernel]\n",
    "                       for kernel in range(kernel_num)])\n",
    "\n",
    "# run the simulation and set inputs\n",
    "previous_spike_count = {}\n",
    "current_spike_count = {}\n",
    "assignments = {}\n",
    "result_monitor = {}\n",
    "results_proportion = {}\n",
    "accuracy = {}\n",
    "\n",
    "previous_spike_count['A1'] = np.zeros(neuron_num)\n",
    "current_spike_count['A1'] = np.zeros(neuron_num)\n",
    "assignments['A1'] = np.zeros(neuron_num)\n",
    "result_monitor['A1'] = np.zeros((validate_interval, neuron_num))\n",
    "results_proportion['A1'] = np.zeros((10, validate_interval))\n",
    "accuracy['A1'] = []\n",
    "input_numbers = np.zeros(validate_interval)\n",
    "\n",
    "neuron_groups['X1'].rates = 0*b.hertz\n",
    "net['M1'].run(0*b.second)\n",
    "\n",
    "start = time.time()\n",
    "j = train_begin\n",
    "input_intensity = start_intensity\n",
    "while j < n_train:\n",
    "\n",
    "    Rates = training['x'][j % 60000, :, :].reshape((n_input)) * input_intensity\n",
    "\n",
    "    neuron_groups['X1'].rates = Rates*b.hertz\n",
    "    connections['X1A1'] = normalize_weights(connections['X1A1'], norm)\n",
    "\n",
    "    net['M1'].run(single_example_time)\n",
    "\n",
    "    current_spike_count['A1'] = np.asarray(\n",
    "        spike_counters['A1'].count[:]) - previous_spike_count['A1']\n",
    "    previous_spike_count['A1'] = np.copy(spike_counters['A1'].count[:])\n",
    "\n",
    "    # if current_spike_count is not enough, increase the input_intensity and simulat this example again\n",
    "    spike_num = np.sum(current_spike_count['A1'])\n",
    "    # print spike_num\n",
    "\n",
    "    if spike_num < retrain_gate:\n",
    "        input_intensity += intensity_step\n",
    "        neuron_groups['X1'].rates = 0*b.hertz\n",
    "        net['M1'].run(resting_time)\n",
    "    else:\n",
    "        result_monitor['A1'][j %\n",
    "                             validate_interval, :] = current_spike_count['A1']\n",
    "        input_numbers[j % validate_interval] = training['y'][j % 60000][0]\n",
    "\n",
    "        neuron_groups['X1'].rates = 0*b.hertz\n",
    "        net['M1'].run(resting_time)\n",
    "        input_intensity = start_intensity\n",
    "\n",
    "        j += 1\n",
    "        if j % progress_interval == 0:\n",
    "            print('Progress: ', j, '/', n_train,\n",
    "                  '(', time.time() - start, 'seconds)')\n",
    "            start = time.time()\n",
    "        if j % validate_interval == 0:\n",
    "            assignments['A1'] = get_new_assignments(\n",
    "                result_monitor['A1'][:], input_numbers[:])\n",
    "            test_results = np.zeros((10, validate_interval))\n",
    "            for k in range(validate_interval):\n",
    "                results_proportion['A1'][:, k] = get_recognized_number_proportion(\n",
    "                    assignments['A1'], result_monitor['A1'][k, :])\n",
    "                test_results[:, k] = np.argsort(\n",
    "                    results_proportion['A1'][:, k])[::-1]\n",
    "            difference = test_results[0, :] - input_numbers[:]\n",
    "            correct = len(np.where(difference == 0)[0])\n",
    "            accuracy['A1'].append(correct/float(validate_interval) * 100)\n",
    "            print('Validate accuracy: ',\n",
    "                  accuracy['A1'][-1], '(last)', np.max(accuracy['A1']), '(best)')\n",
    "\n",
    "        if j % save_interval == 0:\n",
    "            np.save(save_path + 'X1A1' + '_' + str(j), connections['X1A1'].w)\n",
    "            np.save(save_path + 'theta_A1' + '_' +\n",
    "                    str(j), neuron_groups['A1'].theta)\n"
   ]
  }
 ]
}